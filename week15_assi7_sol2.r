# # -*- coding: utf-8 -*-
# """Week15 Assi7 Sol2.ipynb
# 
# Automatically generated by Colaboratory.
# 
# Original file is located at
#     https://colab.research.google.com/drive/1_Et_v9mmzBrmbE8jR1XTGHR_m1d5DieY
# """

###########################################################################
## Week-15, Homework-7, Sol-2 
## Sreya Dhar 
## Created: Dec 09, 2020
## Edited: Dec 16, 2020
###########################################################################

rm(list=ls())
## installing all the libraries in R kernel


# install.packages("corrplot")
# install.packages("forecast")
# install.packages("zoo")
# install.packages("rsample")
# install.packages("leaps")
# install.packages("car")
# install.packages("caret")
# install.packages("ROCR")
# install.packages("PerformanceAnalytics")
# install.packages("funModeling")
# install.packages("hrbrthemes")
# install.packages("ggthemes")
# install.packages("GGally")
# install.packages("glmnet")
# install.packages("ISLR")
# install.packages("kableExtra")
# install.packages("broom")
# install.packages("knitr")
# install.packages("psych")
# install.packages("aod")
# install.packages("epiDisplay")
# install.packages("e1071")
# install.packages("class")
# 
# install.packages("rpart.plot")
# install.packages("party")
# install.packages("partykit")
# install.packages("rattle")

## importing the libraries in R kernel

library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidyr)
library(corrplot)
library(repr)
library(reshape2)
library(forecast)
library(zoo)
library(rsample)
library(gplots)
library(ROCR)
library(class)
library(readr)
library(leaps)
library(car)
library(PerformanceAnalytics)
library(funModeling)
library(gridExtra)
library(caret)
library(MASS)
library(Hmisc)
library(hrbrthemes)
library(GGally)
library(glmnet)
library(pROC)
library(psych)
library(aod)
library(epiDisplay)
library(e1071)
library(ggthemes)
library(kableExtra)
library(broom)
library(knitr)
library(devtools)
library(rpart)  #for trees
library(rattle)    # Fancy tree plot This is a difficult library to install (https://gist.github.com/zhiyzuo/a489ffdcc5da87f28f8589a55aa206dd) 
library(rpart.plot)             # Enhanced tree plots
library(RColorBrewer)       # Color selection for fancy tree plot
library(party)                  # Alternative decision tree algorithm
library(partykit)               # Convert rpart object to BinaryTree
library(randomForest)
library(viridis)
library(tree)
library(factoextra)
library(pca3d)
library(rgl)
library(rrr)

# if (!requireNamespace('BiocManager', quietly = TRUE))
#   install.packages('BiocManager')
# 
# BiocManager::install('PCAtools')

## set directory ##
setwd("C:/File G/EAS 506 Statistical Mining I/Week 15/Assignment-7")

## converting the RData file into .csv file
load("C:/File G/EAS 506 Statistical Mining I/Week 15/Assignment-7/pendigits.RData")
write.csv(pendigits,'pendigits.csv')
pendigits <- read.csv("pendigits.csv", header = TRUE)
digits <- pendigits[,-c(1,19,37)]

digits_final <- digits[, c(1:17)]
digits_features <- digits[,-c(17:34)]
digits_class <- digits[,17]
GGally::ggcorr(digits_final)+theme_bw()

glimpse(digits_final)
names(digits_final)

status(digits_final)

profiling_num(digits_final)

describe(digits_final)

summary(digits_final)
plot_num(digits_final)
# corrplot(digits_final, method = "circle",  type = "lower")

rrr(digits_features, digits_features, type = "pca")$goodness_of_fit
rank_trace(digits_features, digits_features, type = "cva")
rank_trace(digits_features, digits_features, type = "cva", plot = FALSE)
rank_trace(digits_features, digits_features, type = "pca")
rank_trace(digits_features, digits_features, type = "pca", plot = FALSE)


args(pairwise_plot)+theme_bw()
pairwise_plot(digits_features, digits_class, type = "pca")
pairwise_plot(digits_features, digits_class, type = "pca", pair_x = 2, pair_y = 4)
rrr(digits_features, digits_features, type = "pca", rank  = 10)
# args(pca_allpairs_plot)
# pca_allpairs_plot(digits_features, rank = 3, class_labels = digits_class)

## variance of 16 predictor variables ## 
vars <- apply(digits_features, 2, var)
vars
an############### PCA on Whole dataset ################
pca_dig <- prcomp(digits_features, scale = TRUE)
sum_pca_dig <- summary(pca_dig)
sum_pca_dig
options(repr.plot.width=4, repr.plot.height=4, repr.plot.res = 200)
par(mfrow=c(1,1))
fviz_eig(pca_dig, title="Explained variance in whole dataset", xlab="Components")

pca_dig
pairsplot(pca_dig)

### eigen values from PCA on whole dataset
eig_val <- get_eigenvalue(pca_dig)
eig_val

# biplot(pca_dig)
# pca2d(pca_dig, group=digits_class, legend="topleft")
pca2d(pca_dig, group=digits_class, biplot=TRUE, biplot.vars=3, legend=TRUE)


pca3d(pca_dig, group=digits_class, show.ellipses=FALSE,
      ellipse.ci=0.95, show.plane=FALSE, legend="topleft")

### extracting first 5 components from PCA on train and test set ######### 
digits_dat<- digits[,-c(18:34)]
pred_pca_tr <- pca_dig$x[,1:5]
resp_pca_tr <- digits_dat[,17]
dat_pca_tr <- cbind(pred_pca_tr,resp_pca_tr )
data_pca_tr <- data.frame(dat_pca_tr)
pairs.panels(data_pca_tr[,1:5], main = "Pairs plot on PCA components, unclassed on digitclasses", pch = 21, bg = c("blue", "green")[unclass(data_pca_tr$resp_pca_tr)], hist.col="red")

## splitting the dataset into train and test sets
set.seed(4444) ## seeding the sampling
data_split_pca <- initial_split(data_pca_tr, prop = 0.75) ## spliting the data by library 'rsample'
data_train_pca <- training(data_split_pca)
data_test_pca  <- testing(data_split_pca)

### knn of PCA comps ####
accuracy = function(actual, predicted) { ## defining accuracy function 
  mean(actual == predicted)}

error = function(actual, predicted) { ## defining error function 
  mean(actual != predicted)}

iter_k = c(1,3,5,7,9,13,15) ## giving k-values
accu_pca_te = rep(x = 0, times = length(iter_k))
accu_pca_tr = rep(x = 0, times = length(iter_k))

error_pca_te = rep(x = 0, times = length(iter_k))
error_pca_tr = rep(x = 0, times = length(iter_k))

for(i in seq_along(iter_k)) {
  pred_pca_tr = knn( train = data_train_pca[,1:5], 
                     test = data_train_pca[,1:5], 
                     cl = data_train_pca$resp_pca_tr, 
                     k = iter_k[i])
  accu_pca_tr[i] = accuracy(pred_pca_tr,data_train_pca$resp_pca_tr) ## accuracy from knn on train set
  error_pca_tr[i] = error( pred_pca_tr, data_train_pca$resp_pca_tr) ## error from knn on train set
} ## error from knn train set


for(i in seq_along(iter_k)) {
  pred_pca_te = knn( train = data_train_pca[,1:5], 
                     test = data_test_pca[,1:5], 
                     cl = data_train_pca$resp_pca_tr, 
                     k = iter_k[i])
  accu_pca_te[i] = accuracy(pred_pca_te, data_test_pca$resp_pca_tr) ## accuracy from knn on test set
  error_pca_te[i] = error(pred_pca_te, data_test_pca$resp_pca_tr) ## error from knn on test set
  
} ## error from knn test set

error_pca_train <- mean(data_train_pca$resp_pca_tr != pred_pca_tr)
error_pca_test  <- mean(data_test_pca$resp_pca_tr != pred_pca_te)

print(paste('Accuracy of train set from linear regression',(1-error_pca_train)*100,'%'))
print(paste('Accuracy of test set from linear regression',(1-error_pca_test)*100,'%'))

c(error_pca_te, error_pca_tr)

####################### Comparing classification accuracy and errror in knn from PCA components  #########################
options(repr.plot.width=8, repr.plot.height=8, repr.plot.res = 200)
par(mfrow=c(2,2))

# plot accuracy vs choice of k on Training set
plot(iter_k, accu_pca_tr*100, type = "b",col = "blue", cex = 1, pch = 20, lwd = 2, lty = 2,
     xlab = "k, number of neighbors", ylim=c(90,100),
     ylab = "classification accuracy, %", main = "Accuracy of Training set")

abline(v = which.max(accu_pca_tr),y = max(accu_pca_tr)*100, type = "l", col = "red", lwd = 2, lty = 2)
abline(x = which.max(accu_pca_tr),h = max(accu_pca_tr)*100, type = "l", col = "black", lty = 2)


# plot accuracy vs choice of k on Test set
plot(iter_k, accu_pca_te*100, type = "b", col = "blue", cex = 1, pch = 20, lwd = 2,
     
     xlab = "k, number of neighbors", ylab = "classification accuracy, %",
     main = "Accuracy of Test set")

abline(v = 3,y = max(accu_pca_te)*100, type = "l", col = "red", lwd = 2, lty = 2)
abline(x = which.max(accu_pca_te),h = max(accu_pca_te)*100, type = "l", col = "black", lty = 2)

# plot accuracy vs choice of k on Training set
plot(iter_k, error_pca_tr*100, type = "b",col = "blue", cex = 1, pch = 20, lwd = 2, 
     xlab = "k, number of neighbors", 
     ylab = "classification error, %", main = "classification error of Training set")

abline(v = which.min(error_pca_tr),y = min(error_pca_tr)*100, type = "l", col = "red", lwd = 2, lty = 2)
abline(x = which.min(error_pca_tr),h = min(error_pca_tr)*100, type = "l", col = "black", lty = 2)

# # plot accuracy vs choice of k on Test set
plot(iter_k, error_pca_te*100, type = "b", col = "blue", cex = 1, pch = 20, lwd = 2,lty = 2,
     
     xlab = "k, number of neighbors", ylab = "classification error, %",
     main = "classification error of Test set")

abline(v = 3,y = min(error_pca_te)*100, type = "l", col = "red", lwd = 2, lty = 2)
abline(x = which.min(error_pca_te),h = min(error_pca_te)*100, type = "l", col = "black", lty = 2)

## KNN=3 on pca components ##
pred_pca_tr = knn( train = data_train_pca[,1:5], 
                   test = data_train_pca[,1:5], 
                   cl = data_train_pca$resp_pca_tr, 
                   k = 5)
pred_pca_te = knn( train = data_train_pca[,1:5], 
                   test = data_test_pca[,1:5], 
                   cl = data_train_pca$resp_pca_tr, 
                   k = 5)

confusionMatrix(table(actual =data_train_pca$resp_pca_tr, prediction = pred_pca_tr))
confusionMatrix(table(actual =data_test_pca$resp_pca_tr, prediction = pred_pca_te))

################## Preparing the data for Analysis ########################

## splitting the dataset into train and test sets
set.seed(4444) ## seeding the sampling
data_split <- initial_split(digits_final, prop = 0.75) ## spliting the data by library 'rsample'
data_train <- training(data_split)
data_test  <- testing(data_split)


# ##############################################################
# ###################### PCA ###################################
# ############# applying pca on train data #####################
# 
# pca_tr <- prcomp(data_train, scale = TRUE)
# sum_pca_tr  <- summary(pca_tr)
# sum_pca_tr
# 
# ############# applying pca on train data #####################
# pca_te <- prcomp(data_test[,1:12], scale = TRUE)
# sum_pca_te <- summary(pca_te)
# sum_pca_te
# 
# options(repr.plot.width=4, repr.plot.height=4, repr.plot.res = 200)
# par(mfrow=c(1,1))
# fviz_eig(pca_tr, title="Explained variance in train set", xlab="Components")
# 
# pca_tr
# 
# ### eigen values from PCA
# eig_val_tr <- get_eigenvalue(pca_tr)
# eig_val_tr



###################################################################
############### Knn prediction on the dataset #####################
###################################################################

accuracy = function(actual, predicted) { ## defining accuracy function 
  mean(actual == predicted)}

error = function(actual, predicted) { ## defining error function 
  mean(actual != predicted)}

iter_k = c(1,3,5,7,9,13,15) ## giving k-values
accu_pca_te = rep(x = 0, times = length(iter_k))
accu_pca_tr = rep(x = 0, times = length(iter_k))

error_pca_te = rep(x = 0, times = length(iter_k))
error_pca_tr = rep(x = 0, times = length(iter_k))

for(i in seq_along(iter_k)) {
  pred_pca_tr = knn( train = data_train[,1:16], 
                     test = data_train[,1:16], 
                     cl = data_train$V17, 
                     k = iter_k[i])
  accu_pca_tr[i] = accuracy(pred_pca_tr,data_train$V17) ## accuracy from knn on train set
  error_pca_tr[i] = error( pred_pca_tr, data_train$V17) ## error from knn on train set
} ## error from knn train set


for(i in seq_along(iter_k)) {
  pred_pca_te = knn( train = data_train[,1:16], 
                     test = data_test[,1:16], 
                     cl = data_train$V17, 
                     k = iter_k[i])
  accu_pca_te[i] = accuracy(pred_pca_te, data_test$V17) ## accuracy from knn on test set
  error_pca_te[i] = error(pred_pca_te, data_test$V17) ## error from knn on test set
  
} ## error from knn test set

error_pca_train <- mean(data_train$Cover_Type != pred_pca_tr)
error_pca_test  <- mean(data_test$Cover_Type != pred_pca_te)

print(paste('Accuracy of train set from linear regression',(1-error_pca_train)*100,'%'))
print(paste('Accuracy of test set from linear regression',(1-error_pca_test)*100,'%'))

c(error_pca_te, error_pca_tr)

####################### Comparing classification accuracy and errror in knn from PCA components  #########################
options(repr.plot.width=8, repr.plot.height=8, repr.plot.res = 200)
par(mfrow=c(2,2))

# plot accuracy vs choice of k on Training set
plot(iter_k, accu_pca_tr*100, type = "b",col = "blue", cex = 1, pch = 20, lwd = 2, lty = 2,
     xlab = "k, number of neighbors", 
     ylab = "classification accuracy, %", main = "Accuracy of Training set")

abline(v = which.max(accu_pca_tr),y = max(accu_pca_tr)*100, type = "l", col = "red", lwd = 2, lty = 2)
abline(x = which.max(accu_pca_tr),h = max(accu_pca_tr)*100, type = "l", col = "black", lty = 2)


# plot accuracy vs choice of k on Test set
plot(iter_k, accu_pca_te*100, type = "b", col = "blue", cex = 1, pch = 20, lwd = 2,
    
     xlab = "k, number of neighbors", ylab = "classification accuracy, %",
     main = "Accuracy of Test set")

abline(v = 3,y = max(accu_pca_te)*100, type = "l", col = "red", lwd = 2, lty = 2)
abline(x = which.max(accu_pca_te),h = max(accu_pca_te)*100, type = "l", col = "black", lty = 2)

# plot accuracy vs choice of k on Training set
plot(iter_k, error_pca_tr*100, type = "b",col = "blue", cex = 1, pch = 20, lwd = 2, 
     xlab = "k, number of neighbors", 
     ylab = "classification error, %", main = "classification error of Training set")

abline(v = which.min(error_pca_tr),y = min(error_pca_tr)*100, type = "l", col = "red", lwd = 2, lty = 2)
abline(x = which.min(error_pca_tr),h = min(error_pca_tr)*100, type = "l", col = "black", lty = 2)

# # plot accuracy vs choice of k on Test set
plot(iter_k, error_pca_te*100, type = "b", col = "blue", cex = 1, pch = 20, lwd = 2,lty = 2,
     
     xlab = "k, number of neighbors", ylab = "classification error, %",
     main = "classification error of Test set")

abline(v = 3,y = min(error_pca_te)*100, type = "l", col = "red", lwd = 2, lty = 2)
abline(x = which.min(error_pca_te),h = min(error_pca_te)*100, type = "l", col = "black", lty = 2)

## KNN=9 on pca components of ##
pred_pca_tr = knn( train = data_train[,1:16], 
                   test = data_train[,1:16], 
                   cl = data_train$V17, 
                   k = 3)
pred_pca_te = knn( train = data_train[,1:16], 
                   test = data_test[,1:16], 
                   cl = data_train$V17, 
                   k = 3)

confusionMatrix(table(actual =data_train$V17, prediction = pred_pca_tr))
confusionMatrix(table(actual =data_test$V17, prediction = pred_pca_te))

## end ##
