# # -*- coding: utf-8 -*-
# """Week9 Assi5 Sol3.ipynb
# 
# Automatically generated by Colaboratory.
# 
# Original file is located at
#     https://colab.research.google.com/drive/1RGlmDWdK40NQ5ELUnPJWYHgxnPnL8F3n
# """
# 
###########################################################################
## Week-9, Homework-5, Sol-4 (Extra)
## Sreya Dhar 
## Created: Nov 04, 2020
## Edited: Nov 16, 2020
###########################################################################

rm(list=ls())
# ## installing all the libaries in R kernel
# 
# install.packages("corrplot")
# install.packages("forecast")
# install.packages("zoo")
# install.packages("rsample")
# install.packages("leaps")
# install.packages("car")
# install.packages("caret")
# install.packages("ROCR")
# install.packages("PerformanceAnalytics")
# install.packages("funModeling")
# install.packages("hrbrthemes")
# install.packages("ggthemes")
# install.packages("GGally")
# install.packages("glmnet")
# install.packages("ISLR")
# install.packages("kableExtra")
# install.packages("broom")
# install.packages("knitr")
# install.packages("psych")
# install.packages("aod")
# install.packages("epiDisplay")
# install.packages("e1071")
# install.packages("class")
# install.packages('tree')
# install.packages('rpart')
# install.packages('rattle')
# install.packages("partykit")
# install.packages("randomForest")
# install.packages("party")
# install.packages("gbm")

## importing the libraries in R kernel

library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidyr)
library(corrplot)
library(repr)
library(reshape2)
library(forecast)
library(zoo)
library(rsample)
library(gplots)
library(ROCR)
library(class)
library(readr)
library(leaps)
library(car)
library(PerformanceAnalytics)
library(funModeling)
library(caret)
library(MASS)
library(Hmisc)
library(hrbrthemes)
library(GGally)
library(glmnet)
library(pROC)
library(ISLR)
library(psych)
library(aod)
library(epiDisplay)
library(e1071)
library(ggthemes)
library(kableExtra)
library(broom)
library(knitr)
library(rpart)
library(rattle)
library(partykit)
library(randomForest)
library(tree)
library(party)
library(gbm)

## set directory ##
setwd("C:/File E/EAS 506 Statistical Mining I/Week 9/Assignment-5")
# load("C:/File E/EAS 506 Statistical Mining I/Week 9/Assignment-5/vehicle.RData")
# write.csv(vehicle,'vehicle.csv')

## uploading the data
Wine = read.csv('wine.csv',header = TRUE)

glimpse(Wine)

head(Wine)

names(Wine)

status(Wine)

profiling_num(Wine)

status(Wine)

describe(Wine)

summary(Wine)

options(repr.plot.width=12, repr.plot.height=12, repr.plot.res = 230)

pairs.panels(Wine[,2:14], main = "Pairs plot on Wine dataset, unclassed on Wine", pch = 21, bg = c("blue", "green", "red")[unclass(Wine$Wine)], hist.col="red")

# ggpair plot divided into three groups
options(repr.plot.width=10, repr.plot.height=10, repr.plot.res = 200)
ggpairs(Wine, columns = 2:14, ggplot2::aes(colour=Wine))+theme_bw()

options(repr.plot.width=6, repr.plot.height=6, repr.plot.res = 200)
plot_num(Wine)

Wine_n <- Wine %>% mutate_if(is.character, as.factor)
Wine_num <- Wine_n %>% mutate_if(is.factor, as.numeric)

## min-max scaling on Weekly dataset prior to regression
max <- apply(Wine_num, 2 , max)
min <- apply(Wine_num, 2 , min)
Wine_s <- as.data.frame(scale(Wine_num, center = min, scale = max - min))

profiling_num(Wine_s)

# heatmap matrix 
options(repr.plot.width=5, repr.plot.height=5, repr.plot.res = 230)
data_h <- as.data.frame(scale(Wine_s,center=TRUE,scale=TRUE))
heatmap.2(as.matrix(Wine_s), scale = "none", col = bluered(100), trace = "none", density.info = "none")

status(Wine_s)

options(repr.plot.width=6, repr.plot.height=6, repr.plot.res = 200)
plot_num(Wine_s)

## plotting the correlation values on chart matrix which also combined with histogram and scatter plots of different features on scaled dataset.
options(repr.plot.width=10, repr.plot.height=10, repr.plot.res = 200)
chart.Correlation(Wine_s, histogram=TRUE, pch=15)

options(repr.plot.width=6, repr.plot.height=6, repr.plot.res = 200)
L <- cor(Wine_s)
corrplot(L, method = "circle",  type = "lower")

library(tree)

Wine <- Wine %>% mutate_if(is.character, as.factor)

########################################################################
##################### Classification Tree with rpart ###################
########################################################################
# Normalizing the predictor variable 

# ## min-max scaling on vehicle dataset predictors prior to classification
normalized<-function(y) {
  
  x<-y[!is.na(y)]
  
  x<-(x - min(x)) / (max(x) - min(x))
  
  y[!is.na(y)]<-x
  
  return(y)
}

Wine[,2:14]<-apply(Wine[,c(2:14)],2,normalized)
head(Wine)

## splitting the dataset into train and test sets
set.seed(1234) ## seeding the sampling
data_split <- initial_split(Wine, prop = 0.75) ## spliting the data by library 'rsample'
data_train <- training(data_split)
data_test  <- testing(data_split)

############### Classification from Tree library ################
#################################################################
tree_model <- tree(Wine ~ ., data=data_train, split = "deviance")
# tree_model

summary(tree_model)

options(repr.plot.width=8, repr.plot.height=8, repr.plot.res = 200)
plot(tree_model)
text(tree_model)

# Distributional prediction on train set
tree_pred_tr <- predict(tree_model, data_train, type="class") # gives the probability for each class

confusionMatrix(table(tree_pred_tr, data_train$Wine))

# Distributional prediction on test set
tree_pred <- predict(tree_model, data_test, type="class") # gives the probability for each class

confusionMatrix(table(tree_pred, data_test$Wine))

tree_pred

options(repr.plot.width=5, repr.plot.height=6, repr.plot.res = 200)
plot(Wine$Mg, Wine$Proline,pch=19,col=as.numeric(Wine$Wine))
# partition.tree(tree_model,label="Wine",add=TRUE)
# legend(1.75,4.5,legend=unique(Wine$Wine),col=unique(as.numeric(Wine$Wine)),pch=19)

# Another way to show the data:
options(repr.plot.width=5, repr.plot.height=4, repr.plot.res = 200)
plot(Wine$Color.int, Wine$Flavanoids, pch=19, col=as.numeric(Wine$Wine))
# partition.tree(tree_model, label="Wine", add=TRUE)
# legend("topright",legend=unique(Wine$Wine), col=unique(as.numeric(Wine$Wine)), pch=19)

pruned_tree <- prune.tree(tree_model, best=3)
options(repr.plot.width=7, repr.plot.height=9, repr.plot.res = 200)
plot(pruned_tree)
text(pruned_tree)

pruned_predict_tr <- predict(pruned_tree, data_train, type="class") # give the predicted class
confusionMatrix(table(Prediction=pruned_predict_tr, Reference=data_train$Wine))

pruned_prediction <- predict(pruned_tree, data_test, type="class") # give the predicted class
confusionMatrix(table(Prediction=pruned_prediction, Reference=data_test$Wine))

pruned_prediction

options(repr.plot.width=5, repr.plot.height=5, repr.plot.res = 200)
cv_model <- cv.tree(tree_model, FUN = prune.misclass)
plot(cv_model)

cv_model$dev  # gives the deviance for each K (small is better)

best_size <- cv_model$size[which(cv_model$dev==min(cv_model$dev))] # which size is better?
best_size

# let's refit the tree model (the number of leafs will be no more than best.size)
cv_model_pruned <- prune.misclass(tree_model, best=best_size)
summary(cv_model_pruned)

options(repr.plot.width=5, repr.plot.height=7, repr.plot.res = 200)
plot(cv_model_pruned)
text(cv_model_pruned)

pruned_tr_cv <- predict(cv_model_pruned, data_train, type="class") # give the predicted class
confusionMatrix(table(Prediction=pruned_tr_cv, Reference=data_train$Wine))

pruned_pred_cv <- predict(cv_model_pruned, data_test, type="class") # give the predicted class
confusionMatrix(table(Prediction=pruned_pred_cv, Reference=data_test$Wine))

##########################################################################
##################### Classification Tree with rpart #####################
##########################################################################

options(repr.plot.width=8, repr.plot.height=8, repr.plot.res = 200)
model.control <- rpart.control(minsplit = 5, cp = 0, xval = 5)
rpart_tree <- rpart(Wine ~ ., data=data_train, control =model.control)
plot(rpart_tree, uniform=TRUE, branch=0.6, margin=0.05)
text(rpart_tree, all=TRUE, use.n=TRUE)
title("Classification Tree on Train set")

# summary(rpart_tree )

options(repr.plot.width=7, repr.plot.height=7, repr.plot.res = 200)
# plot decision tree
fancyRpartPlot(rpart_tree, main="Wine")

## rpart predition for train set
rpart_pred_tr <- predict(rpart_tree, data_train, type="class")
confusionMatrix(table(data_train$Wine, rpart_pred_tr))

rpart_pred <- predict(rpart_tree, data_test, type="class")
confusionMatrix(table(data_test$Wine, rpart_pred))

options(repr.plot.width=4, repr.plot.height=4, repr.plot.res = 200)
plotcp(rpart_tree) # visualize cross-validation results

options(repr.plot.width=4, repr.plot.height=4, repr.plot.res = 200)
plot(rpart_tree$cptable[,4], main = "Cp for model selection", ylab = "cv error", type='l', ylim=c(0.1,1.2))
points(rpart_tree$cptable[,4], col="red", cex=1,pch=20)

min_cp = which.min(rpart_tree$cptable[,4])
prune_rpart<- prune(rpart_tree, cp=rpart_tree$cptable[min_cp, 1])
 # pruning the tree
options(repr.plot.width=8, repr.plot.height=10, repr.plot.res = 200)
plot(prune_rpart, uniform=TRUE, branch=0.6)
text(prune_rpart, all=TRUE, use.n=TRUE)

rpart_pr_pred_tr <- predict(prune_rpart, data_train, type="class")
confusionMatrix(table(data_train$Wine, rpart_pr_pred_tr))

rpart_pr_pred_te <- predict(prune_rpart, data_test, type="class")
confusionMatrix(table(data_test$Wine, rpart_pr_pred_te))

###########################################################################
################ Conditional Inference Tree ###############################
###########################################################################

lmat <- matrix(c(0,100,100,
                 1,0,0,
                 2,0,0), ncol = 3)
lmat

rpart.tree <- rpart(Wine ~ ., data=data_train, parms = list(loss = lmat))
predictions <- predict(rpart.tree, data_test, type="class")
rpart_tab_1 <- table(Reference=data_test$Wine, Prediction=predictions)
confusionMatrix(table(predictions, data_test$Wine))

options(repr.plot.width=8, repr.plot.height=6, repr.plot.res = 200)
plot(rpart.tree)
text(rpart.tree)

#### variable importance plot from rpart #####

varimp_rpart<- data.frame( imp= rpart_tree$variable.importance)
varimp_rpart_plot <- varimp_rpart %>% 
  tibble::rownames_to_column() %>% 
  dplyr::rename("variable" = rowname) %>% 
  dplyr::arrange("Importance"=imp) %>%
  dplyr::mutate(variable = forcats::fct_inorder(variable))

options(repr.plot.width=5, repr.plot.height=5, repr.plot.res = 200)

ggplot2::ggplot(varimp_rpart_plot) +
  geom_segment(aes(x = variable, y = 0, xend = variable, yend = imp), 
               size = 1.5, alpha = 0.7) +
  geom_point(aes(x = variable, y = imp, col = variable), 
             size = 4, show.legend = F) +
  coord_flip() + labs(title = "Variable Importance from rpart tree")
  theme_bw()

rparty_tree <- as.party(rpart_tree)
rparty_tree

options(repr.plot.width=10, repr.plot.height=5, repr.plot.res = 200)
plot(rparty_tree)

########################################
### Conditional Inference Tree ###
########################################
options(repr.plot.width=10, repr.plot.height=6, repr.plot.res = 200)
inf_model <- ctree(Wine ~ . , data = data_train)
plot(inf_model)

inf_pred_tr<- predict(inf_model, data_train[,-1])
inf_tab_tr <- table(Prediction=inf_pred_tr, Reference=data_train$Wine)
caret::confusionMatrix(inf_tab_tr, positive = "Barbera")

inf_pred<- predict(inf_model, data_test[,-1])
inf_tab <- table(Prediction=inf_pred, Reference=data_test$Wine)
caret::confusionMatrix(inf_tab, positive = "Barbera")

# get the probabilities from the barplots showen above:
tapply(treeresponse(inf_model), where(inf_model), unique)

# The package is able to format the plot tree. Eg:
innerWeights <- function(node){
  grid.circle(gp = gpar(fill = "White", col = 1))
  mainlab <- paste( node$psplit$variableName, "\n(n = ")
  mainlab <- paste(mainlab, sum(node$weights),")" , sep = "")
  grid.text(mainlab,gp = gpar(col='red'))
}

options(repr.plot.width=11, repr.plot.height=5, repr.plot.res = 200)
plot(inf_model, type='simple', inner_panel = innerWeights)

##############################################################
#################### Random Forest ###########################
##############################################################
rf <- randomForest(Wine ~., data=data_train, importance=TRUE, do.trace=100, ntree=500, mtry = 4)
print(rf)

varImpPlot(rf, main="Variable Importance Accuracy and Gini coeff. from Random Forest")

importance(rf)

## Prediction on train set
rf_pred_tr <- predict(rf, data_train, type = "response")
rf_tab <- confusionMatrix(table(data_train$Wine, rf_pred_tr))
# caret::confusionMatrix(rf_tab, positive = "Barbera")
rf_tab

## Prediction on test set
rf_predict <- predict(rf, data_test,type = "response")
rf_tab <- confusionMatrix(table(data_test$Wine, rf_predict))
# caret::confusionMatrix(rf_tab, positive = "Barbera")
rf_tab

options(repr.plot.width=4, repr.plot.height=4, repr.plot.res = 200)
# next function gives a graphical depiction of the marginal effect of a variable on the class probability (classification) 
partialPlot(rf, data_train, Mg, "Barbera")

rf_tree <- getTree(rf, k=2) # show the second tree
print(rf_tree)

treesize(rf) # size of trees of the ensemble

mean(treesize(rf))

options(repr.plot.width=4, repr.plot.height=4, repr.plot.res = 200)
hist(treesize (rf), col = "red")

library("e1071") # to 'tune' the rf model

tuned_rf <- tune(randomForest, train.x = Wine ~ ., data = data_train, validation.x = data_test)

best_rf <- tuned_rf$best.model
best_rf_pred <- predict(best_rf, data_test, type='response')
tab_best_rf <- table(Reference=data_test$Wine, Prediction=best_rf_pred)
confusionMatrix(tab_best_rf)

tuned_rf

best_rf

# computing overall error:
err_rf <- 1 - sum(diag(as.matrix(tab_best_rf))) / sum(tab_best_rf)
err_rf

###############################################
# Boosting ##
###############################################
mod_boost = gbm(Wine ~.,
              data = data_train,
              distribution = "multinomial",
              cv.folds = 10,
              shrinkage = .1,
              n.minobsinnode = 10,
              n.trees = 200)

print(mod_boost)

pred_boost_tr = predict.gbm(object = mod_boost,
                            newdata = data_train,
                            n.trees = 200,
                            type = "response")

labels_tr = colnames(pred_boost_tr)[apply(pred_boost_tr, 1, which.max)]
result_tr = data.frame(data_train$Wine, labels_tr)
confusionMatrix(data_train$Wine, as.factor(labels_tr))

pred_boost_te = predict.gbm(object = mod_boost,
                   newdata = data_test,
                   n.trees = 200,
                   type = "response")


labels = colnames(pred_boost_te)[apply(pred_boost_te, 1, which.max)]
result = data.frame(data_test$Wine, labels)
# print(result)

confusionMatrix(data_test$Wine, as.factor(labels))


####################################
## Linear Discriminant Analysis (LDA)
####################################

## splitting the scaled dataset into train and test sets for LDA
set.seed(1234) ## seeding the sampling
data_split <- initial_split(Wine_s, prop = 0.75) ## spliting the data by library 'rsample'
data_train <- training(data_split)
data_test  <- testing(data_split)

data_train[,1] -> y_true_train
data_test[,1] -> y_true_test


dim(data_train)

## fitting the model ##
lda_mod <- lda(Wine ~., data = data_train)
lda.pred.train <- predict(lda_mod, newdata = data_train)
y_lda_train <- lda.pred.train$class
lda.pred.test <- predict(lda_mod, newdata = data_test)
y_lda_test <- lda.pred.test$class

# Compute the LDA error
lda_train_error <- mean(y_true_train  != y_lda_train)
lda_test_error <- mean(y_true_test != y_lda_test)

## Misclassifiaction error for LDA
lda_train_error
lda_test_error

lda_mod

summary(lda.pred.train$class)

## AUC for LDA

roc_lda_tr <- multiclass.roc(data_train$Wine, lda.pred.train$posterior)
roc_lda_te <- multiclass.roc(data_test$Wine, lda.pred.test$posterior)

print("LDA train set AUC")
auc(roc_lda_tr)
print("LDA test set AUC")
auc(roc_lda_te)

## confusion matrix from LDA on train set

tab_tr_lda <- table(Predicted=y_lda_train, Reference=data_train$Wine )
colnames(tab_tr_lda) = c("Barbera", "Barolo", "Grignolino")
rownames(tab_tr_lda) = c("Barbera", "Barolo", "Grignolino")
caret::confusionMatrix(tab_tr_lda, positive = "Barbera")

## probability table

round(prop.table(caret::confusionMatrix(tab_tr_lda)$table), 2)

## confusion matrix from LDA on test set

tab_te_lda <- table(Predicted=y_lda_test, Reference=data_test$Wine)
colnames(tab_te_lda) = c("Barbera", "Barolo", "Grignolino")
rownames(tab_te_lda) = c("Barbera", "Barolo", "Grignolino")
caret::confusionMatrix(tab_te_lda, positive = "Overt_Diabetic")

## probability table

conf_mat_lda <- round(prop.table(caret::confusionMatrix(tab_te_lda)$table), 2)
conf_mat_lda

## probability table
round(prop.table(caret::confusionMatrix(tab_te_lda)$table), 2)

options(repr.plot.width=3, repr.plot.height=6, repr.plot.res = 250)
par(mfrow = c(1,1))

ldahist(lda.pred.train$x[,1], g= y_lda_train, col="red", main="Wine vs. coeff. plot from LDA on train set")

## plot for LDA components ###
options(repr.plot.width=3, repr.plot.height=6, repr.plot.res = 250)
par(mfrow = c(1,1))
ldahist(lda.pred.test$x[,1], g= y_lda_test, col="green")

LD_func_test <- as.data.frame(lda.pred.test$x)
Wine_orig_test <- factor(data_test$Wine, 
                          labels = c("Barbera", "Barolo", "Grignolino"))

LD_func_train <- as.data.frame(lda.pred.train$x)
Wine_orig_train <- factor(data_train$Wine, 
                           labels = c("Barbera", "Barolo", "Grignolino"))

options(repr.plot.width=7, repr.plot.height=5, repr.plot.res = 250)
ggplot(LD_func_test , aes(x=LD1, y=LD2, color=Wine_orig_test)) + 
  geom_point(size=2) +
  theme_bw()

options(repr.plot.width=7, repr.plot.height=5, repr.plot.res = 250)
ggplot(LD_func_train , aes(x=LD1, y=LD2, color=Wine_orig_train)) + 
  geom_point(size=2) +
  theme_bw()

## end ##