# # -*- coding: utf-8 -*-
# """Week7 Assi4 Sol2.ipynb
# 
# Automatically generated by Colaboratory.
# 
# Original file is located at
#     https://colab.research.google.com/drive/17nzEsTkxHhmRRyFcSSGe8lXdXxAuMBrt
# """

rm(list = ls())

## installing all the libaries in R kernel

# install.packages("corrplot")
# install.packages("forecast")
# install.packages("zoo")
# install.packages("rsample")
# install.packages("leaps")
# install.packages("car")
# install.packages("caret")
# install.packages("ROCR")
# install.packages("PerformanceAnalytics")
# install.packages("funModeling")
# install.packages("hrbrthemes")
# install.packages("ggthemes")
# install.packages("GGally")
# install.packages("glmnet")
# install.packages("ISLR")
# install.packages("kableExtra")
# install.packages("broom")
# install.packages("knitr")
# install.packages("psych")
# install.packages("aod")
# install.packages("epiDisplay")
# install.packages("e1071")
# install.packages("class")

## importing the libraries in R kernel

library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidyr)
library(corrplot)
library(repr)
library(reshape2)
library(forecast)
library(zoo)
library(rsample)
library(gplots)
library(ROCR)
library(class)
library(readr)
library(leaps)
library(car)
library(PerformanceAnalytics)
library(funModeling)
library(caret)
library(MASS)
library(Hmisc)
library(hrbrthemes)
library(GGally)
library(glmnet)
library(pROC)
library(ISLR)
library(psych)
library(aod)
library(epiDisplay)
library(e1071)
library(ggthemes)
library(kableExtra)
library(broom)
library(knitr)

# Set working directory to where data file is located
setwd("C:/File E/EAS 506 Statistical Mining I/Week 7/Assignment-4")

## upload the dataset from ISLR package
data('Weekly')

glimpse(Weekly)

head(Weekly)

names(Weekly)

glimpse(Weekly)

status(Weekly)

profiling_num(Weekly)

status(Weekly)

describe(Weekly)

summary(Weekly)

options(repr.plot.width=6, repr.plot.height=6, repr.plot.res = 230)

pairs.panels(Weekly[,2:7], main = "Pairs plot on Weekly dataset, unclassed on Direction", pch = 21, bg = c("blue", "green")[unclass(Weekly$Direction)], hist.col="red")

# ggpair plot divided into two groups
options(repr.plot.width=10, repr.plot.height=10, repr.plot.res = 200)
ggpairs(Weekly, columns = 2:7, ggplot2::aes(colour=Direction))+theme_bw()


## Converting the Direction variable into numerical one

data <- Weekly %>% mutate_if(is.factor, as.numeric)-1
data_1<- data[,2:9]
data_1<- data_1[,-7]
status(data_1)

profiling_num(data_1)

# heatmap matrix 
options(repr.plot.width=5, repr.plot.height=5, repr.plot.res = 230)
data_h <- as.data.frame(scale(data_1,center=TRUE,scale=TRUE))
heatmap.2(as.matrix(data_h), scale = "none", col = bluered(100), trace = "none", density.info = "none")

## min-max scaling on Weekly dataset prior to regression
max <- apply(data_1, 2 , max)
min <- apply(data_1, 2 , min)
data_s <- as.data.frame(scale(data_1, center = min, scale = max - min))

options(repr.plot.width=6, repr.plot.height=6, repr.plot.res = 200)
plot_num(data_s)

## splitting the dataset into train and test sets
set.seed(1205) ## set unique seed 
data_split <- initial_split(data_s, prop = 0.75) ## spliting the data by library 'rsample'
data_train <- training(data_split)
data_test  <- testing(data_split)


#########################################################################################
##########################   Logistic Regression ########################################
#########################################################################################

logit_full <- glm(Direction ~ . , data = data_train, family = 'binomial')
summary(logit_full)

## CIs using profiled log-likelihood gor Logistic Regression

confint(logit_full)

## CIs using standard errors

confint.default(logit_full)

wald.test(b = coef(logit_full), Sigma = vcov(logit_full), Terms = 1:5)

## odds ratios and 95% CI

logistic.display(logit_full)

##  the difference in deviance for the two models (i.e., the test statistic)

with(logit_full, null.deviance - deviance)

## the difference of DOFs between the two models

with(logit_full, df.null - df.residual)

## p-value of the logit model

with(logit_full, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))

## log-likelyhood of the model

logLik(logit_full)

## Predition on test set from full logistic regression model ##

pred_logit_full <- predict(logit_full, newdata = data_test, type = 'response')
pred_val_logit_full <- ifelse(pred_logit_full >= 0.5, 1, 0)
acc_logit_full <- paste('Accuracy:', round(mean(pred_val_logit_full == data_test$Direction),2))
acc_logit_full ## accuracy on test set

## confusion matrix from logit reg. on test set

tab_te_logit_full <- table(Predicted=pred_val_logit_full, Reference=data_test$Direction)
colnames(tab_te_logit_full) = c("Up", "Down")
rownames(tab_te_logit_full) = c("Up", "Down")
caret::confusionMatrix(tab_te_logit_full, positive = "Up")

## probability table of confusion matrix

round(prop.table(caret::confusionMatrix(tab_te_logit_full)$table), 2)

## baseline dividing two groups
baseline_logit_full <- mean(data_train$Direction == 1)
baseline_logit_full

### Adding the year variable in the dataset

data_s["Year"] <-Weekly$Year 

## Preparing the train and test set for the 2nd logit model where Lag2 is the onlt predictor

train_lag2 <- data_s[data_s$Year <= 2008,]
test_lag2 <- data_s[data_s$Year > 2008,]

dim(train_lag2)
dim(test_lag2)

#########################################################################################
#####  Logistic Regression where Lag2 is the only predictor and the dataset has been 
#####  divided based on year
#########################################################################################

lag2_logit <- glm(Direction ~ Lag2, data = train_lag2, family = 'binomial')
summary(lag2_logit)

## odds ratios and 95% CI

logistic.display(lag2_logit)

## p-value of the logit model

with(lag2_logit, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))

##  Predition on test set from Lag2 logistic regression model ##

pred_lag2_logit <- predict(lag2_logit, newdata = test_lag2, type = 'response')
pred_val_lag2_logit <- ifelse(pred_lag2_logit >= 0.5, 1, 0)

acc_lag2_logit <- paste('Accuracy:', mean(pred_val_lag2_logit == test_lag2$Direction))
acc_lag2_logit

## confusion matrix from lag2 logit reg. on test set

tab_te_lag2_logit <- table(Predicted=pred_val_lag2_logit, Reference=test_lag2$Direction)
colnames(tab_te_lag2_logit) = c("Up", "Down")
rownames(tab_te_lag2_logit) = c("Up", "Down")
caret::confusionMatrix(tab_te_lag2_logit, positive = "Up")

## probability table of confusion matrix

round(prop.table(caret::confusionMatrix(tab_te_lag2_logit)$table), 2)

baseline_lag2_logit <- mean(test_lag2$Direction == 1)
baseline_lag2_logit

length(test_lag2$Lag2)

## Logistic Regression plot from Lag2_logit model
options(repr.plot.width=4, repr.plot.height=4, repr.plot.res = 200)
newdat <- data.frame(Lag2=seq(min(test_lag2$Lag2), max(test_lag2$Lag2),len=length(test_lag2$Lag2)))
newdat$Direction = predict(lag2_logit, newdata=newdat, type="response")
plot(Direction~Lag2, data=test_lag2, col="red4")
lines(Direction~Lag2, newdat, col="green4", lwd=2)

################################################################################################
###  LDA Regression where Lag2 is the only predictor and the dataset has been divided based on year
################################################################################################

### KNN on train set ###
pred_lda_tr <- predict(lda_model, newdata = train_lag2)
pred_val_lda_tr <- pred_lda_tr$class

acc_lda_tr <- paste('Accuracy:', mean(pred_val_lda_tr == train_lag2$Direction))


acc_lda_tr

## confusion matrix from lag2 as predictor on LDA model on train set

tab_tr_lda <- table(Predicted=pred_val_lda_tr, Reference=train_lag2$Direction)
colnames(tab_tr_lda) = c("Up", "Down")
rownames(tab_tr_lda) = c("Up", "Down")
caret::confusionMatrix(tab_tr_lda, positive = "Up")


### KNN on test set ###
lda_model <- lda(Direction ~ Lag2, data = train_lag2)
pred_lda <- predict(lda_model, newdata = test_lag2)
pred_val_lda <- pred_lda$class

acc_lda <- paste('Accuracy:', mean(pred_val_lda == test_lag2$Direction))
acc_lda

## confusion matrix from lag2 as predictor on LDA model on test set

tab_te_lda <- table(Predicted=pred_val_lda, Reference=test_lag2$Direction)
colnames(tab_te_lda) = c("Up", "Down")
rownames(tab_te_lda) = c("Up", "Down")
caret::confusionMatrix(tab_te_lda, positive = "Up")

## probability table of confusion matrix for LDA

round(prop.table(caret::confusionMatrix(tab_te_lda)$table), 2)

#########################################################################################
###  KNN with k=1 where Lag2 is the only predictor and the dataset has been divided based 
###  on year
#########################################################################################

### KNN on train set ###

knn_lag2_tr <- knn(train = data.frame(train_lag2$Lag2), 
                   test = data.frame(train_lag2$Lag2), 
                   cl = train_lag2$Direction, k = 1)

acc_knn_tr <- paste('Accuracy:', round(mean(knn_lag2_tr == train_lag2$Direction),2))

acc_knn_tr

## confusion matrix from lag2 as predictor on KNN model on train set (k=1)

tab_tr_knn <- table(Predicted=knn_lag2_tr, Reference=train_lag2$Direction)
colnames(tab_tr_knn) = c("Up", "Down")
rownames(tab_tr_knn) = c("Up", "Down")
caret::confusionMatrix(tab_tr_knn, positive = "Up")

### KNN on test set ###
knn_lag2 <- knn(train = data.frame(train_lag2$Lag2), 
                test = data.frame(test_lag2$Lag2), 
                cl = train_lag2$Direction, k = 1)

acc_knn <- paste('Accuracy:', round(mean(knn_lag2 == test_lag2$Direction),2))
acc_knn

## confusion matrix from lag2 as predictor on KNN model on test set (k=1)

tab_te_knn <- table(Predicted=knn_lag2, Reference=test_lag2$Direction)
colnames(tab_te_knn) = c("Up", "Down")
rownames(tab_te_knn) = c("Up", "Down")
caret::confusionMatrix(tab_te_knn, positive = "Up")

## probability table of confusion matrix for Knn: k=1

round(prop.table(caret::confusionMatrix(tab_te_knn)$table), 2)


#########################################################################################
########################  ROC AUC plot for comparison  ###############################
#########################################################################################

## get AUC ROC for test set
auc(pred_val_logit_full, data_test$Direction) ## full_logit_auc 
auc(pred_val_lag2_logit, test_lag2$Direction) ## Lag2_logit_auc 
auc(pred_val_lda, test_lag2$Direction) ## LDA_auc 
auc(knn_lag2, test_lag2$Direction) ## KNN_auc (k=1) 

### Defining the plot ###
pred_full_logit_auc <- prediction(data.frame(pred_val_logit_full), data_test$Direction)
perform_full_logit <- performance(pred_full_logit_auc, measure = "tpr", x.measure = "fpr")

pred_lag2_logit_auc <- prediction(data.frame(pred_val_lag2_logit), test_lag2$Direction)
perform_lag2_logit <- performance(pred_lag2_logit_auc, measure = "tpr", x.measure = "fpr")

pred_lda_auc <- prediction(data.frame(as.numeric(pred_val_lda)), test_lag2$Direction)
perform_lda <- performance(pred_lda_auc, measure = "tpr", x.measure = "fpr")

pred_knn_auc <- prediction(as.numeric(knn_lag2), test_lag2$Direction)
perform_knn <- performance(pred_knn_auc, measure = "tpr", x.measure = "fpr")

options(repr.plot.width=8, repr.plot.height=8, repr.plot.res = 250)
plot(perform_full_logit,  ylim=c(0,1), col="red", type = "l")
plot(perform_lag2_logit,  add=TRUE, col="brown", type = "l") ## plot ROC
plot(perform_lda,  add=TRUE, col="blue", type = "l") ## plot ROC
plot(perform_knn,  add=TRUE, col="green", type = "l") ## plot ROC

abline(a=0, b=1, type='l', lwd=1, lty=2, col="black")
legend("bottomright", 95, legend=c("full logistic", "Lag2 Logit", "LDA", "KNN(k=1)",  "diagonal"), col=c("red", "brown", "blue", "green",  "black" ), lty=1:2, cex=0.8,box.lty=2, box.lwd=2, box.col="green")


#########################################################################################
############ Knn prediction on the full dataset #########################################
#########################################################################################

accuracy = function(actual, predicted) { ## defining accuracy function 
  mean(actual == predicted)}

error = function(actual, predicted) { ## defining error function 
  mean(actual != predicted)}

iter_k = c(1,3,5,7,9,11,13,15) ## giving k-values
accu_pca_te = rep(x = 0, times = length(iter_k))
accu_pca_tr = rep(x = 0, times = length(iter_k))

error_pca_te = rep(x = 0, times = length(iter_k))
error_pca_tr = rep(x = 0, times = length(iter_k))

for(i in seq_along(iter_k)) {
  pred_pca_tr = knn( train = data_train[,1:6], 
                test = data_train[,1:6], 
                cl = data_train$Direction, 
                k = iter_k[i])
  accu_pca_tr[i] = accuracy(pred_pca_tr,data_train$Direction) ## accuracy from knn on train set
  error_pca_tr[i] = error( pred_pca_tr, data_train$Direction) ## error from knn on train set
} ## error from knn train set

############ Knn prediction on the full dataset for test set ########################

for(i in seq_along(iter_k)) {
  pred_pca_te = knn( train = data_train[,1:6], 
              test = data_test[,1:6], 
              cl = data_train$Direction, 
              k = iter_k[i])
  accu_pca_te[i] = accuracy(pred_pca_te, data_test$Direction) ## accuracy from knn on test set
  error_pca_te[i] = error(pred_pca_te, data_test$Direction) ## error from knn on test set
  
} ## error from knn test set


error_pca_train <- mean(data_train$Direction != pred_pca_tr)
error_pca_test  <- mean(data_test$Direction != pred_pca_te)

print(paste('Accuracy of train set from linear regression',(1-error_pca_train)*100,'%'))
print(paste('Accuracy of test set from linear regression',(1-error_pca_test)*100,'%'))

c(error_pca_te, error_pca_tr)

####################### Comparing classification accuracy and errror in knn from PCA components  #########################
options(repr.plot.width=8, repr.plot.height=8, repr.plot.res = 200)
par(mfrow=c(2,2))

# plot accuracy vs choice of k on Training set
plot(iter_k, accu_pca_tr*100, type = "b",col = "blue", cex = 1, pch = 20, lwd = 2, lty = 2,
     xlab = "k, number of neighbors", ylim= c(40,100),
     ylab = "classification accuracy, %", main = "Accuracy of Training set")

abline(v = which.max(accu_pca_tr),y = max(accu_pca_tr)*100, type = "l", col = "red", lwd = 2, lty = 2)
abline(x = which.max(accu_pca_tr),h = max(accu_pca_tr)*100, type = "l", col = "black", lty = 2)


# plot accuracy vs choice of k on Test set
plot(iter_k, accu_pca_te*100, type = "b", col = "blue", cex = 1, pch = 20, lwd = 2,
     ylim= c(40,100),
     xlab = "k, number of neighbors", ylab = "classification accuracy, %",
     main = "Accuracy of Test set")

abline(v = 7,y = max(accu_pca_te)*100, type = "l", col = "red", lwd = 2, lty = 2)
abline(x = which.max(accu_pca_te),h = max(accu_pca_te)*100, type = "l", col = "black", lty = 2)

# plot accuracy vs choice of k on Training set
plot(iter_k, error_pca_tr*100, type = "b",col = "blue", cex = 1, pch = 20, lwd = 2, 
     xlab = "k, number of neighbors", ylim=c(0,60),
     ylab = "classification error, %", main = "classification error of Training set")

abline(v = which.min(error_pca_tr),y = min(error_pca_tr)*100, type = "l", col = "red", lwd = 2, lty = 2)
abline(x = which.min(error_pca_tr),h = min(error_pca_tr)*100, type = "l", col = "black", lty = 2)

# # plot accuracy vs choice of k on Test set
plot(iter_k, error_pca_te*100, type = "b", col = "blue", cex = 1, pch = 20, lwd = 2,lty = 2,
     ylim=c(0,60),
     xlab = "k, number of neighbors", ylab = "classification error, %",
     main = "classification error of Test set")

abline(v = 7,y = min(error_pca_te)*100, type = "l", col = "red", lwd = 2, lty = 2)
abline(x = which.min(error_pca_te),h = min(error_pca_te)*100, type = "l", col = "black", lty = 2)

#########################################################################################
############ Knn prediction only with lag2 predictor ####################################
#########################################################################################

accuracy = function(actual, predicted) { ## defining accuracy function 
  mean(actual == predicted)}

error = function(actual, predicted) { ## defining error function 
  mean(actual != predicted)}


iter_k = c(1,3,5,7,9,11,13,15) ## giving k-values
accu_pca_te = rep(x = 0, times = length(iter_k))
accu_pca_tr = rep(x = 0, times = length(iter_k))

error_pca_te = rep(x = 0, times = length(iter_k))
error_pca_tr = rep(x = 0, times = length(iter_k))

for(i in seq_along(iter_k)) {
  pred_pca_tr = knn( train = data.frame(train_lag2$Lag2), 
                     test = data.frame(train_lag2$Lag2), 
                     cl = train_lag2$Direction, 
                     k = iter_k[i])
  accu_pca_tr[i] = accuracy(pred_pca_tr,train_lag2$Direction) ## accuracy from knn on train set
  error_pca_tr[i] = error( pred_pca_tr, train_lag2$Direction) ## error from knn on train set
} ## error from knn train set


for(i in seq_along(iter_k)) {
  pred_pca_te = knn( train = data.frame(train_lag2$Lag2), 
                     test = data.frame(test_lag2$Lag2), 
                     cl = train_lag2$Direction, 
                     k = iter_k[i])
  accu_pca_te[i] = accuracy(pred_pca_te, test_lag2$Direction) ## accuracy from knn on test set
  error_pca_te[i] = error(pred_pca_te, test_lag2$Direction) ## error from knn on test set
  
} ## error from knn test set


error_pca_train <- mean(train_lag2$Direction != pred_pca_tr)
error_pca_test  <- mean(test_lag2$Direction != pred_pca_te)

print(paste('Accuracy of train set from linear regression',(1-error_pca_train)*100,'%'))
print(paste('Accuracy of test set from linear regression',(1-error_pca_test)*100,'%'))

c(error_pca_te, error_pca_tr)

#### plot on KNN ###

####################### Comparing classification accuracy and errror in knn from PCA components  #########################
options(repr.plot.width=8, repr.plot.height=8, repr.plot.res = 200)
par(mfrow=c(2,2))

# plot accuracy vs choice of k on Training set
plot(iter_k, accu_pca_tr*100, type = "b",col = "blue", cex = 1, pch = 20, lwd = 2, lty = 2,
     xlab = "k, number of neighbors", ylim= c(40,100),
     ylab = "classification accuracy, %", main = "Accuracy of Training set")

abline(v = which.max(accu_pca_tr),y = max(accu_pca_tr)*100, type = "l", col = "red", lwd = 2, lty = 2)
abline(x = which.max(accu_pca_tr),h = max(accu_pca_tr)*100, type = "l", col = "black", lty = 2)


# plot accuracy vs choice of k on Test set
plot(iter_k, accu_pca_te*100, type = "b", col = "blue", cex = 1, pch = 20, lwd = 2,
     ylim= c(40,100),
     xlab = "k, number of neighbors", ylab = "classification accuracy, %",
     main = "Accuracy of Test set")

abline(v = 13,y = max(accu_pca_te)*100, type = "l", col = "red", lwd = 2, lty = 2)
abline(x = which.max(accu_pca_te),h = max(accu_pca_te)*100, type = "l", col = "black", lty = 2)

# plot accuracy vs choice of k on Training set
plot(iter_k, error_pca_tr*100, type = "b",col = "blue", cex = 1, pch = 20, lwd = 2, 
     xlab = "k, number of neighbors", ylim=c(0,60),
     ylab = "classification error, %", main = "classification error of Training set")

abline(v = which.min(error_pca_tr),y = min(error_pca_tr)*100, type = "l", col = "red", lwd = 2, lty = 2)
abline(x = which.min(error_pca_tr),h = min(error_pca_tr)*100, type = "l", col = "black", lty = 2)

# # plot accuracy vs choice of k on Test set
plot(iter_k, error_pca_te*100, type = "b", col = "blue", cex = 1, pch = 20, lwd = 2,lty = 2,
     ylim=c(0,60),
     xlab = "k, number of neighbors", ylab = "classification error, %",
     main = "classification error of Test set")

abline(v = 13,y = min(error_pca_te)*100, type = "l", col = "red", lwd = 2, lty = 2)
abline(x = which.min(error_pca_te),h = min(error_pca_te)*100, type = "l", col = "black", lty = 2)



### end ###