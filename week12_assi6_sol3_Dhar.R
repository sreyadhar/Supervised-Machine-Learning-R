# # -*- coding: utf-8 -*-
# """Week12 Assi6 Sol3.ipynb
# 
# Automatically generated by Colaboratory.
# 
# Original file is located at
#     https://colab.research.google.com/drive/1ipr8vvSqjkmrYGeMwu8Z_u77HvyWBkGp
# """
###########################################################################
## Week-12, Homework-6, Sol-3 
## Sreya Dhar 
## Created: Nov 24, 2020
## Edited: Dec 3, 2020
###########################################################################

## installing all the libaries in R kernel

# install.packages("corrplot")
# install.packages("forecast")
# install.packages("zoo")
# install.packages("rsample")
# install.packages("leaps")
# install.packages("car")
# install.packages("caret")
# install.packages("ROCR")
# install.packages("PerformanceAnalytics")
# install.packages("funModeling")
# install.packages("hrbrthemes")
# install.packages("ggthemes")
# install.packages("GGally")
# install.packages("glmnet")
# install.packages("ISLR")
# install.packages("kableExtra")
# install.packages("broom")
# install.packages("knitr")
# install.packages("psych")
# install.packages("aod")
# install.packages("epiDisplay")
# install.packages("e1071")
# install.packages("class")
# install.packages('tree')
# install.packages('rpart')
# install.packages('rattle')
# install.packages("partykit")
# install.packages("randomForest")
# install.packages("party")
# install.packages("kernlab")
# """# New Section"""

## importing the libraries in R kernel

library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidyr)
library(corrplot)
library(repr)
library(reshape2)
library(forecast)
library(zoo)
library(rsample)
library(gplots)
library(ROCR)
library(class)
library(readr)
library(leaps)
library(car)
library(PerformanceAnalytics)
library(funModeling)
library(caret)
library(MASS)
library(Hmisc)
library(hrbrthemes)
library(GGally)
library(glmnet)
library(pROC)
library(ISLR)
library(psych)
library(aod)
library(epiDisplay)
library(e1071)
library(ggthemes)
library(kableExtra)
library(broom)
library(knitr)
library(rpart)
library(rattle)
library(partykit)
library(randomForest)
library(tree)
library(party)
library(kernlab)
library(tree)

set.seed(12345)

data(spam)
head(spam)

glimpse(spam)

names(spam)

status(spam)

profiling_num(spam)

describe(spam)

summary(spam)

########################################################################
##################### Classification Tree with rpart ###################
########################################################################
# Normalizing the predictor variable 

# ## min-max scaling on vehicle dataset predictors prior to classification
normalized<-function(y) {
  
  x<-y[!is.na(y)]
  
  x<-(x - min(x)) / (max(x) - min(x))
  
  y[!is.na(y)]<-x
  
  return(y)
}

spam[,1:57]<-apply(spam[,c(1:57)],2,normalized)
head(spam)

## splitting the dataset into train and test sets
set.seed(1234) ## seeding the sampling
data_split <- initial_split(spam, prop = 0.75) ## spliting the data by library 'rsample'
data_train <- training(data_split)
data_test  <- testing(data_split)



##############################################################
#################### Random Forest ###########################
##############################################################
rf <- randomForest(type ~., data=data_train, importance=TRUE, do.trace=100, ntree=500, mtry = 4)
print(rf)

install.packages("plotmo")
library(plotmo)

options(repr.plot.width=8, repr.plot.height=8, repr.plot.res = 200)
plotres(rf)

rf$err.rate[500]

options(repr.plot.width=10, repr.plot.height=10, repr.plot.res = 200)
varImpPlot(rf, main="Variable Importance Accuracy and Gini coeff. from Random Forest")

importance(rf)

install.packages("pdp")
library(pdp)

options(repr.plot.width=7, repr.plot.height=5, repr.plot.res = 250)
par.ped.age <- partial(rf, pred.var = c("capitalAve", "all"), chull = TRUE)
plot.ped.age <- autoplot(par.ped.age, contour = TRUE, 
               legend.title = "Partial\ndependence")

grid.arrange(plot.ped.age )

options(repr.plot.width=7, repr.plot.height=5, repr.plot.res = 250)
par.ped.age <- partial(rf, pred.var = c("charExclamation", "all"), chull = TRUE)
plot.ped.age <- autoplot(par.ped.age, contour = TRUE, 
               legend.title = "Partial\ndependence")

grid.arrange(plot.ped.age )

options(repr.plot.width=6, repr.plot.height=6, repr.plot.res = 230)
par(mfrow=c(2,2))
# next function gives a graphical depiction of the marginal effect of a variable on the class probability (classification) 
partialPlot(rf, data_train, charExclamation, "spam")
partialPlot(rf, data_train, your, "spam")
partialPlot(rf, data_train, all, "spam")
partialPlot(rf, data_train, remove, "spam")

## Prediction on train set
rf_pred_tr <- predict(rf, data_train, type = "response")
rf_tab <- confusionMatrix(table(data_train$type, rf_pred_tr))
rf_tab

## Prediction on test set
rf_predict <- predict(rf, data_test,type = "response")
rf_tab <- confusionMatrix(table(data_test$type, rf_predict))
rf_tab

rf_tree <- getTree(rf, k=2) # show the second tree
# print(rf_tree)

treesize(rf) # size of trees of the ensemble

mean(treesize(rf))

options(repr.plot.width=4, repr.plot.height=4, repr.plot.res = 200)
hist(treesize (rf), col = "red")

######################################### Iteration on mtry for RF #########################################

rf.c <- list()
oob.err <- list()
yhat.rf <-list()
misclass_rf<-list()
for ( i in 1:57 ) {
  set.seed(4444)
  rf.c<-randomForest(type ~ ., data = data_train, mtry = i, importance = TRUE, ntree = 500)
  oob.err[i] = rf.c$err.rate[500] #Error of 500th Tree fitted
  yhat.rf<-predict(rf.c, newdata = data_test[,-58])
  misclass_rf[i]<- mean(yhat.rf != data_test$type)
}

options(repr.plot.width=6, repr.plot.height=4, repr.plot.res = 200)
matplot(1:57, misclass_rf, xlab = 'No. of Variables (mtry)', ylab = 'Misclassification error', main = "Subset size Vs. Misclassification error")
lines(1:57, misclass_rf, type = "o")

options(repr.plot.width=9, repr.plot.height=7, repr.plot.res = 200)
matplot(1:57 , cbind(oob.err, misclass_rf), pch=19 , col=c("red","blue"), type=c("o","o"), ylab="Misclassification Error rate", xlab="Number of Predictors Considered at each Split, mtry")
legend("topright", legend=c("Out of Bag Error","Test Error"), pch=19, col=c("red","blue"))

options(repr.plot.width=8, repr.plot.height=8, repr.plot.res = 230)
plotmo(rf,  pmethod="partdep", all1=TRUE)

# options(repr.plot.width=10, repr.plot.height=10, repr.plot.res = 230)
# plotmo(rf,  pmethod="partdep", all1=TRUE, all2=TRUE)

# misclass_rf ## min for mtry=4,5 on test set

################ Tuning on RF model ###############

library("e1071") # to 'tune' the rf model

tuned_rf <- tune(randomForest, train.x = type ~ ., data = data_train, validation.x = data_test, mtry=4, ntree=500)

best_rf <- tuned_rf$best.model
best_rf_pred <- predict(best_rf, data_test, type='response')
tab_best_rf <- table(Reference=data_test$type, Prediction=best_rf_pred)
confusionMatrix(tab_best_rf)

tuned_rf

best_rf

options(repr.plot.width=10, repr.plot.height=5, repr.plot.res = 200)
par(mfrow=c(1,2))
plot(rf, type = "l", ylim=c(0.01,.12))
legend("topright", 95, legend=c("OOB Error", "non-spam oob error", "spam oob error"), col=c("black", "green", "red"))
plot(best_rf, type = "l", ylim=c(0.01,.12))

print(rf)

# computing overall error:
err_rf <- 1 - sum(diag(as.matrix(tab_best_rf))) / sum(tab_best_rf)
err_rf

rf_predict_num <- as.numeric(rf_predict)-1
rf_true_num <- as.numeric(data_test$type)-1
brf_predict_num <- as.numeric(best_rf_pred)-1

#########################################################################################
########################  ROC AUC plot for comparison  ##################################
#########################################################################################

## get AUC ROC for test set
auc(rf_predict_num, rf_true_num) ## random Forest
auc(brf_predict_num, rf_true_num) ## random Forest
### Defining the plot ###
pred_rf_auc <- prediction(data.frame(rf_predict_num), rf_true_num)
perform_rf <- performance(pred_rf_auc, measure = "tpr", x.measure = "fpr")
pred_brf_auc <- prediction(data.frame(brf_predict_num), rf_true_num)
perform_brf <- performance(pred_brf_auc, measure = "tpr", x.measure = "fpr")

options(repr.plot.width=7, repr.plot.height=5, repr.plot.res = 250)
plot(perform_rf,  ylim=c(0,1), col="red", type = "l", colorize=TRUE)
plot(perform_brf,  add=TRUE, col="blue", type = "l")
abline(a=0, b=1, type='l', lwd=1, lty=2, col="black")
legend("bottomright", 95, legend=c("random forest", "best tuned rf"), col=c("red", "blue"), lty=1:2, cex=1.0, box.lty=2, box.lwd=2, box.col="green")


### end ###